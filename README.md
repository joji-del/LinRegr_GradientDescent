# LinearRegression - GradientDescent

Добро пожаловать в **LinearRegression - GradientDescent** — репозиторий, содержащий Jupyter Notebook, в котором исследуется реализация и применение градиентного спуска для линейной регрессии с различными функциями потерь, включая среднеквадратичную ошибку (MSE), MSE с L2-регуляризацией и Huberloss. Этот проект создан в образовательных целях, предоставляя практический опыт работы с оптимизацией градиентного спуска и реализацией функций потерь на Python.

## Обзор

Репозиторий включает Jupyter Notebook (`ML-2.ipynb`), который охватывает следующие темы:
- Реализация градиентного спуска для линейной регрессии.
- Пользовательские функции потерь: MSE, MSE с L2-регуляризацией и HuberLoss.
- Методы предобработки данных, такие как обработка пропущенных значений и кодирование категориальных признаков.
- Визуализация траекторий градиентного спуска для различных значений шага обучения.
- Сравнение производительности моделей с разными функциями потерь на обучающей и тестовой выборках.
- Использование библиотеки scikit-learn для сравнения результатов и подбора гиперпараметров.

Ноутбук хорошо документирован и включает математические формулировки, код реализации и визуализации для лучшего понимания градиентного спуска и его поведения.

## Требования

Для работы с ноутбуком вам понадобятся следующие библиотеки Python:
- `numpy`
- `pandas`
- `matplotlib`
- `scikit-learn`
- `scipy`

Вы можете установить их с помощью команды:
```bash
pip install numpy pandas matplotlib scikit-learn scipy
```

## Структура репозитория

- `ML-2.ipynb`: Основной Jupyter Notebook с реализацией и экспериментами.
- `README.md`: Этот файл, содержащий описание проекта.

## Использование

1. Склонируйте репозиторий:
   ```bash
   git clone https://github.com/<your-username>/GradientDescentPlayground.git
   ```
2. Перейдите в папку репозитория:
   ```bash
   cd GradientDescentPlayground
   ```
3. Запустите Jupyter Notebook:
   ```bash
   jupyter notebook ML-2.ipynb
   ```
4. Следуйте инструкциям в ноутбуке, чтобы изучить реализацию градиентного спуска, экспериментировать с параметрами и визуализировать результаты.

## Основные разделы ноутбука

1. **Реализация функций потерь**:
   - Класс `MSELoss` для среднеквадратичной ошибки.
   - Класс `MSEL2Loss` для MSE с L2-регуляризацией.
   - Класс `HuberLoss` для функции потерь Хубера.

2. **Градиентный спуск**:
   - Реализация функции `gradient_descent` для оптимизации весов модели.
   - Визуализация траекторий градиентного спуска с помощью функции `plot_gd`.

3. **Предобработка данных**:
   - Обработка пропущенных значений с использованием моды.
   - Кодирование категориальных признаков с помощью `LabelEncoder` и `get_dummies`.
   - Стандартизация признаков с использованием `StandardScaler`.

4. **Обучение и сравнение моделей**:
   - Обучение линейной регрессии с разными функциями потерь.
   - Подбор гиперпараметров (например, коэффициента регуляризации) с помощью `GridSearchCV`.
   - Сравнение производительности моделей на обучающей и тестовой выборках с использованием метрики MSE.

## Пример использования

```python
# Создание синтетического датасета
X = np.random.uniform(-5, 5, (300, 2))
y = X.dot(np.random.normal(size=(2,))) + np.random.normal(0, 1, (300,))
w_init = np.random.uniform(-2, 2, (2,))

# Обучение модели с MSELoss
loss = MSELoss()
w_list = gradient_descent(w_init, X, y, loss, lr=0.01, n_iterations=100)
plot_gd(w_list, X, y, loss)
```

## Результаты

- Исследовано влияние шага обучения (`lr`) на сходимость градиентного спуска.
- Проведено сравнение моделей с MSE, MSE с L2-регуляризацией и Huber Loss.
- Подобран оптимальный коэффициент регуляризации для `MSEL2Loss` с использованием `GridSearchCV`.
- Продемонстрирована устойчивость Huber Loss к выбросам по сравнению с MSE.


